# ğŸ§  Python Coding Test Answer Generator

> LeetCode ë¬¸ì œë¥¼ ì…ë ¥í•˜ë©´, ê°€ë…ì„±ê³¼ íš¨ìœ¨ì„±ì´ ë†’ì€ íŒŒì´ì¬ ëª¨ë²”ë‹µì•ˆì„ ìƒì„±í•˜ëŠ” sLLM íŒŒì¸íŠœë‹ í”„ë¡œì íŠ¸

---

## ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

### ë°°ê²½
  ì½”ë”©í…ŒìŠ¤íŠ¸ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ ì±„ìš©ì—ì„œ í•µì‹¬ì ì¸ í‰ê°€ ìš”ì†Œì´ë©°, LeetCodeì™€ ê°™ì€ ì˜¨ë¼ì¸ ì €ì§€ì˜ ë¬¸ì œë“¤ì´ ì‚¬ì‹¤ìƒ í‘œì¤€ì²˜ëŸ¼ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤.
### ëª©í‘œ
  LeetCode ë¬¸ì œ ë° í’€ì´ ë°ì´í„°ë¥¼ í™œìš©í•´ â€œíŒŒì´ì¬ ê¸°ë°˜ ì½”ë”©í…ŒìŠ¤íŠ¸ ëª¨ë²”ë‹µì•ˆ ìƒì„±ëª¨ë¸" ê°œë°œ
  ë‹¨ìˆœíˆ ì •ë‹µë§Œ ë§íˆëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê°€ë…ì„±(PEP8)ê³¼ íš¨ìœ¨ì„±(time/memory) ì„ í•¨ê»˜ ê³ ë ¤í•œ ì½”ë“œ ìƒì„±ì´ ëª©í‘œ
### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

  * sLLM íŒŒì¸íŠœë‹
  * SFT (Supervised Fine-Tuning)
  * DPO (Direct Preference Optimization)
  * (ì„ íƒ) ORPO, KTO, GRPO ë“± ìµœì‹  preference alignment ê¸°ë²•

---

## ğŸ“‚ ë°ì´í„°ì…‹

| ë°ì´í„°ì…‹                 | ì„¤ëª…                           | ì¶œì²˜                                                                                     |
| -------------------- | ---------------------------- | -------------------------------------------------------------------------------------- |
| **LeetCode Dataset** | ë¬¸ì œ ì„¤ëª… + ì œì•½ ì¡°ê±´ â†’ Python ì •ë‹µ ì½”ë“œ | [newfacade/LeetCodeDataset](https://huggingface.co/datasets/newfacade/LeetCodeDataset) |
| **APPS (ë³´ì¡° ë°ì´í„°)**    | í”„ë¡œê·¸ë˜ë° ë¬¸ì œì™€ ì •ë‹µ ì½”ë“œ ìŒ            | [codeparrot/apps](https://huggingface.co/datasets/codeparrot/apps)                     |

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

* **Framework**: PyTorch, Hugging Face Transformers
* **Training**: PEFT, TRL
* **Optimization**: LoRA / QLoRA, Flash-Attn, Gradient Checkpointing
* **Deployment**: vLLM (Page Attention), KV Cache
* **Evaluation**: Gemini / GPT ê¸°ë°˜ ì½”ë“œ í’ˆì§ˆ í‰ê°€

---

## ğŸ§© ë°©ë²•ë¡ 

### 1. Supervised Fine-Tuning (SFT)

* ì…ë ¥: LeetCode ë¬¸ì œ ì„¤ëª…
* ì¶œë ¥: ì •ë‹µ ì½”ë“œ + ì½”ë“œ ì„¤ëª…
* ëª¨ë¸ì´ ì¶œë ¥ íŒ¨í„´ê³¼ í•¨ìˆ˜ êµ¬ì¡°ë¥¼ í•™ìŠµ

### 2. Direct Preference Optimization (DPO)

* ë™ì¼ ë¬¸ì œì— ëŒ€í•œ ë‘ ê°œì˜ ë‹µì•ˆ ì½”ë“œ ë¹„êµ

  * ì˜ˆ: ì •ë‹µì´ì§€ë§Œ ë¹„íš¨ìœ¨ì ì¸ ì½”ë“œ vs ë” ìµœì í™”ëœ ì½”ë“œ
* ëª¨ë¸ì´ ì™„ì„±ë„ ë†’ì€ ì½”ë“œë¥¼ ì„ íƒí•˜ë„ë¡ í•™ìŠµ

### 3. ORPO (Optional)

* Odds Ratio ê¸°ë°˜ preference í•™ìŠµ
* ê°€ë…ì„± ë° ì½”ë“œ ê°„ê²°ì„± ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ í•™ìŠµ
* SFT & DPOë¥¼ í•˜ë‚˜ì˜ training processì—ì„œ í•´ê²°

---

## âš™ï¸ ì‹¤í—˜ í™˜ê²½

| í•­ëª©          | êµ¬ì„±                                         |
| ----------- | ------------------------------------------ |
| **GPU**     | RunPod A100 (40GB)                         |
| **Python**  | 3.11.10                                    |
| **PyTorch** | 2.4                                        |
| **Model**   | `Qwen2.5-3B-Instruct` |
| **ì§€ì› ê¸°ëŠ¥**   | QLoRA, Flash-Attn, Gradient Checkpointing  |

---

## ğŸ” ì°¸ê³  ìë£Œ

* [TRL - Transformer Reinforcement Learning](https://huggingface.co/docs/trl/index)
* [LLM RLHF ê¸°ë²• ì •ë¦¬ (PPO, DPO, IPO, KTO, ORPO, GRPO)](https://davidlds.tistory.com/100)
* [A Systematic Survey of Prompt Engineering (2024)](https://arxiv.org/abs/2402.07927)
* [ORPO ë…¼ë¬¸ ë¦¬ë·°](https://meanwo0603.tistory.com/entry/ORPO-Monolithic-Preference-Optimization-without-Reference-Model-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0)
* [RunPod ì„œë²„ ëŒ€ì—¬ ë° VS CODE ì—°ë™](https://velog.io/@lse0912/RunPod-%EC%84%9C%EB%B2%84-%EB%8C%80%EC%97%AC-%EB%B0%8F-VS-CODE-%EC%97%B0%EB%8F%99#runpod%EB%9E%80)

---

## ğŸ§© Repository Structure

```bash
.
â”œâ”€â”€ checkpoints         # weights of trained model
â”œâ”€â”€ config.json         
â”œâ”€â”€ main.ipynb          # pipeline from data preparation to SFT/DPO
â”œâ”€â”€ make_prompts.py     # prompts for training
â”œâ”€â”€ qlora.py            # includes functions to load model and tokenizer for QLoRA manner
â””â”€â”€ readme.md
```